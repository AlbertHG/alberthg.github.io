---
layout:     post
title:      "Machine Learning Yearning:Chapter 2"
subtitle:   《Machine Learning Yearning》翻译之“配置开发集和训练集”
date:       2018-04-23
author:     ATuk
header-img: img/ML_yearning1.jpg
catalog: true
tags:
    - maching learning yearning
    - 机器学习
    - 策略
    - Andrew NG
    - 翻译
---

## 前言

> 人工智能、机器学习和深度学习正在越来越多的行业发挥着重要的作用。领域大牛吴恩达最近又有了小动作——正在完成一本开源的关于机器学习策略的手册，为各路道友提供构建机器学习项目的指导。根据NG的介绍，本书重点不是ML的算法，而是如何使ML算法发挥作用。琳琅满目的ML算法就像是工具箱里边的各种工具一样，这本书则是教会人们如何使用这些工具。笔者将对NG的这本 ~~葵花宝典~~ 武林秘籍进行翻译，本篇博客是“第二章：配置开发集和训练集”，欢迎提出建议。   👉[官网传送门](http://www.mlyearning.org/)

## 5. 你的开发集和测试集

让我们回到一开始那个猫咪图片识别的例子来。你投入运营了一个手机App，用户可以上传很多不同类型的图片到你的App来。此时，你希望在App上实现一个自动识别猫咪图片的功能。

你的团队通过从不同的网站抓取到很多猫图（正例）和非猫图（反例）组成了一个很大的数据集，然后按照70%/30%的法则将数据集分为了训练集和测试集两部分。通过使用这些数据，团队实现了一个在训练集和测试集上均表现良好的猫咪检测器。

但是，当你将这个检测器部署到手机App上的时候，发现它的性能变得相当糟糕！

这……发生了什么？

你发现用户上传的图片数据和团队从网站上抓取下来作为训练集的图片数据存在差异：用户上传的图片普遍使用手机拍摄，这些图片的分辨率比较低，清晰度差甚至曝光不足，然而，你的训练集/测试集上的图片则是来自网页抓取的图片。因此，你的分类器算法并不能很好的泛化到App的实际场景中——识别来自手机拍摄的图片。

在大数据时代来临之前，使用70%/30%的比例将数据集随机分割成训练集和测试集是传统机器学习的通用法则。这种做法是可行的，但是，当越来越多的应用部署在与训练集（来自网页抓取的图片）有着不同分布的使用场景（用户手机拍摄的图片）中的时候，这种分割做法就比较糟糕了。

我们通常定义：

- 训练集(Training Set)：对算法或者模型进行训练所使用的数据集；
- 开发集(Dev(development) Set​​)：用于调整参数，选择特征和做出其他算法相关决定的数据集，又称作“交叉验证集”(Hold-Out Cross Validation Set)；
- 测试集(Test set​​)：只用来评估算法性能而不会对使用何种算法或者参数做出决策的数据集。

一旦定义好了开发集和测试集之后，你的团队将会尝试很多新的想法，例如，设置不同的学习算法参数来看一下哪种效果最佳。总之，开发集和测试集的使用可以让你的团队对算法调优进行快速迭代。

换句话说，*设置开发集和测试集的目的是指导你的团队对机器学习系统做出最正确的优化。*

因此，你应该遵从下列原则：

- 选择能够映射出你在未来将要获得的数据，且表现出良好效果的开发集和测试集。

也就是说，你的测试集不应该仅仅包含现阶段可用数据的30%，特别是当你期望得到的数据（用户的手机拍摄的图片）和你的训练集的数据（网站抓取的图片）来自不同分布的时候。

如果你还没有上线你的App，那么你可能还没有任何用户，因此无法获取符合未来数据分布的图片，但是您仍然可以尝试去模拟这种分布。比如，请求你的朋友们使用手机拍些猫咪的图片给你。一旦应用上线，就可以使用用户上传的数据去更新你的开发集和测试集了。

如果真的没有任何办法去获取那些符合未来数据分布的图片的话，也许你可以使用上边提到的网站抓取的图片来优化算法，但是应该清醒地意识到这种方式训练出来的系统的泛化(Generalize)性能肯定是不好的。

你需要有一定的判断力来决定投入多少预算去提高开发集和测试集的质量。切记不要假定你的训练集和测试集有着相同的分布，尝试挑选那些能够很好的体现出你最终想要的结果的数据作为测试样本，而不仅仅是挑那些和训练集有着相同分布的数据。


## 6. 开发集和测试集应当服从同一分布

根据你的App的受众地区，将猫咪数据来源分为四个区域：(i)美国、(ii)中国，(iii)印度和(iv)其他。我们提出了一个服从这样分布(Distribution)的开发集和测试集：将美国和印度地区的数据归入开发集中，将来自中国和其他地区的数据归入测试集中。简单说就是，我们随机将两个地区的数据分配给开发集，另外两个分配给测试集。这种分法对吗？

错啦！！！

一旦定义好开发集和测试集，你的团队将会专注于提高开发集上的表现。所以，开发集应该要反映出你最想要改进的任务——在四个区域都表现良好，而不是其中的两个。

开发集和测试集服从不同分布导致的第二个问题是：你的团队可能会构建出来在开发集上表现良好的系统，但发现它在测试集中却表现很差，我曾在大量的挫折和错误的努力中得到过这种结果。现在要避免这种悲剧发生在你身上。

还是以例子来说明，假设你的团队构建的系统在开发集上表现良好但是在测试集却很糟糕。如果此时你的开发集和测试集服从同一个分布，那么你将非常清楚的知道哪里出了问题——数据过拟合了。一个有效的办法就是获取更多的开发集数据。

但是，如果开发集和测试集服从不同的分布，那么你的决策就不那么明朗了，以下几个方面都可能是出错的地方：

1. 你过拟合(Overfit)了开发集；
2. 测试集的数据比开发集数据要更加复杂，你的算法已经达到了预期的效果并且已经无法进一步改善了；
3. 测试集的数据并不比开发集数据复杂，只是因为服从不同分布，所以开发集上良好的性能表现并不能泛化到训练集中。在这种情况下，你在开发集上所做的努力就全部白费了。

开发机器学习应用已经足够困难，不匹配的开发集和测试集则引入了额外的不确定性——改进开发集的分布是否能提高测试集表现？这会使得更加难以确定哪些优化措施是有效的哪些是徒劳的，从而难以指定优化措施的优先级顺序。

如果你处理第三方的基准测试，它们的创建者可能指定了来自不同分布的开发集和训练集。与服从同一分布的开发集和训练集相比，对于基准测试的性能表现，运气将会比技巧产生更大的影响。当然，构建能够在一种分布中表现良好而且能泛化到其他分布的学习算法是很重要的研究方向。但是，如果你的目标是构建出能在特定的机器学习应用中取得进展的话，我建议你尝试选择服从同一分布的开发集和测试集，这会使您的团队更有效率。

## 7. 开发集/测试集多大合适？

开发集应该足够大，大到能够检测出你在尝试的不同算法之间的差异。比如，如果分类器A的准确率有90%，分类器B的准确率有90.1%，那么只有100个样本的开发集是无法检测到这0.1%的差异的。与我所见过的机器学习问题相比，100样本大小的开发集着实很小。样本规模在1000-10000之间的开发集很常见，通过10000样本大小的开发集，你才有更大的可能性检测出这0.1%的提高[^1]。

[^1]:理论上，我们还可以在开发集上测试算法的改变能否产生统计意义上的显著差异。事实上，大多数团队不会为此而操心（除非他们正在准备发表学术论文），而且我通常没有发现对测量中期进展有用的统计显着性检验。

对于那些成熟的，重要的应用来说，比如那些广告、搜索引擎和产品推荐等，我也看到为了提升那0.01%而不懈努力的团队，这个努力将直接影响公司的利润。在这种情况下，为了能够检测到更加小算法性能改进，开发集的大小可能远大于10000。

那么测试集的大小多少合适呢？他应该足够大，大到能够为系统的整体性能提供很高的可信度。一种流行的启发性方法就是将数据集的30%用于训练集，这适用于一个具有中等大小规模（100-10000个样本）的数据集。但是在大数据时代，我们所面临的可能是有着数十亿样本的机器学习问题，分配给开发/测试集的数据比例一直在降低，但是开发/测试集的数据量却是不断递增的。总之，在给开发/测试集分配数据时，没有必要分配超出评估算法所需的数据量。

## 8. 为团队进行算法优化建立单一数字评估指标

分类准确率是单一数字评估指标(Single-Number Evaluation Metric​​)的其中一种：你在开发集（测试集）上运行你的分类器，然后得到一个对样本分类准确率的一个数字，通过这个指标，如果分类器A的准确率是97%，而分类器B的准确率是90%，则我们判定分类器A更好。

相比之下，查准率(Precision)和查全率(Recall)[^2]就不属于单一数字评估指标：它给出了两个数字用来评估分类器。使用混合数字评估指标(Multiple-Number Evaluation Metrics)使得比较算法的差异变得更加困难。假设你的算法表现如下表：

[^2]:对于猫分类器这个案例。查准率的定义是：分类器将开发（测试）集标记为猫的样本中，有多少真的是猫，$\frac{预测为正类的正类数量}{预测为正类的数量} * 100%$；查全率就是在开发（测试）集中，分类器正确识别出的真是猫的图片数量占所有的真是猫的图片数量的百分比,$\frac{预测为正类的正类数量}{正类数量} * 100%$。通常需要在高查准率与高查全率之间进行权衡。

分类器|查准率|查全率
---|---|---
A|95%|90%
B|98%|85%

上边的举例中，任意一个分类器都没有明显表现的比另一个好，所以这类混合数字评估指标无法立刻引导你选择哪一个分类器更好。

在开发过程中，你的团队会进行大量关于算法架构、模型参数、特征选择方面的尝试。使用单一数字评估指标，比如准确率，可以允许你依照该指标下的性能表现对所有模型进行排序，从而快速的决断出哪一个模型效果更好。

如果你希望综合考虑查准率、查全率的性能度量，我建议使用一种标准的方法将它们组合成一个单一数字性能指标。例如，可以将查准率和查全率的均值作为评价指标。或者你可以计算“F1度量”(F1 score)，这是一种基于查准率和查全率的调和平均，比单纯的去两者的均值效果要好[^3]。

[^3]:如果你想更多的了了解关于“F1度量”的信息，请参阅：[https://en.wikipedia.org/wiki/F1_score](https://en.wikipedia.org/wiki/F1_score),其计算公式：$F1 = \frac{2}{\frac{1}{P}+\frac{1}{R} } = \frac{2PR}{P+R}$

分类器|查准率|查全率|F1度量
---|---|---|---
A|95%|90%|92.4%
B|98%|85%|91.0%

当你要在大量分类器中做出选择的时候，单一数字评估指标能帮助你更快的做出决策。它可以为算法表现提供明确的优先级，从而给出一个清晰的优化方向。

作为最后一个例子，假设你已经得到了你的猫分类器在四个主要市场（美国、印度、中国和其他）的分类准确率。这给出了四个指标，通过将这四个地区的分类准确率进行平均或者加权平均，最终将得到一个单一数字指标。取均值或者加权平均是将多个指标合并为一个的最常用的办法之一。

-----------

🚧🚧（正在施工）🚧🚧
