---
layout:     post
title:      《机器学习要领》 偏差和方差（中文翻译版）
subtitle:   Machine Learning Yearning Chapter4 Bias and Variance(Chinese ver)
date:       2018-05-02
author:     Canary
header-img: img/ML_yearning1.jpg
catalog: true
tags:
    - maching learning yearning
    - 机器学习
    - Andrew NG
    - 翻译
---

## 前言

> 人工智能、机器学习和深度学习正在越来越多的行业发挥着重要的作用。领域大牛吴恩达最近又有了小动作——正在完成一本开源的关于机器学习策略的手册，为各路道友提供构建机器学习项目的指导。根据NG的介绍，本书重点不是ML的算法，而是如何使ML算法发挥作用。琳琅满目的ML算法就像是工具箱里边的各种工具一样，这本书则是教会人们如何使用这些工具。笔者将对NG的这本 ~~葵花宝典~~ 武林秘籍进行翻译，本篇博客是“第四章：偏差和方差”，欢迎提出建议。   
👉[官网传送门](http://www.mlyearning.org/)<br>
👉[GitHub项目传送门](https://github.com/AlbertHG/Machine-Learning-Yearning-Chinese-ver)，欢迎Star

## 20. 偏差和方差：两大误差来源

假设你的训练、开发和测试集都服从同一分布，那么你应该总是试图去获取更多的训练数据，因为这能提高你的系统性能，对吗？

尽管，有大量可供获取的数据并没有坏处，但不幸的是，它并不是总能像你期望的那样给你带来帮助，有时候只顾着获取更多的数据只会是浪费时间。那么该如何决定什么时候添加数据什么时候不添加数据呢？

在机器学习中，有两个主要的误差(Error)来源：偏差(Bias)和方差(Variance)。了解这两个指标能够省时省力地帮助我们作出是否添加数据或者其它能够提高系统性能的策略决定。

假设你希望构建一个只有5%误差的猫咪识别器。目前，你的训练集误差有15%，而开发集的误差有16%。这种情况下，往训练集塞更多的数据用处不大。此时的你应该关注其他变化。事实上，在你的训练集上增加更多的样本数据只会让你的算法在训练集上表现的越来越好而已。（我们会在后边的章节解释原因！）

如果当前你的算法在训练集上的误差是15%（准确率85%），而你的目标是降低误差到5%（准确率95%），因此首要的问题就是提高算法在训练集上的性能表现。算法在开发集和测试集的性能往往要低于训练集，也就是说，如果你的算法在你已有的数据（可以理解为训练集上）上的准确率都只有85%的话，那么想要算法在未见过的数据（可以理解为在开发集上）中达到95%的准确率无异于是天方夜谭。

假设你的算法在开发集上的误差是16%（准确率84%），通常将16%的误差分为两个部分：

- 首先，算法在训练集上的误差，在此例中是15%。我们非正式地把它看作是算法的偏差(Bias);
- 其次，算法在开发集上的表现和训练集上的表现的差值（开发集误差-训练集误差），在此例中，开发集和训练集的准确度差值为1%。我们非正式地把它看作是算法的方差(Variance)[^1]。
 
[^1]: 在统计学领域，对于方差和偏差有更正式的定义。但大致上可以将偏差定义为当你有一个很大的训练集的时候算法在训练集上的误差；将方差定义为算法在测试集表现和训练集表现的差值。当你的误差指标是均方误差时，你可以写出指定这两个量的公式，并证明“总体误差=偏差+方差”。但为了简化叙述如何通过对偏差和方差的分析来解决机器学习问题，这里给出非正式定义的偏差和方差就足够了。

学习算法的一些优化措施能够解决误差的第一部分——偏差——提高算法在训练集上的表现。有一些优化措施能够解决误差的第二部分——方差——帮助算法顺利地从训练集推广到开发/测试集上[^2]。为了选择出最能提升算法性能表现的优化方向，深刻理解错误的两个部分的优先级顺序是很有必要的。

[^2]:还有一些方法可以通过对系统架构进行重大改动来同时减少方差和偏差，但这往往操作难度很大。

培养关于偏差和方差的良好直觉能够帮助你为算法选择有效的优化措施。

## 21. 举例说明偏差和方差

考虑我们的猫分类任务。 一个“理想”的分类器（比如人）在这项任务中可能会取得近乎完美的表现。

假设你的算法表现如下：

- 训练集误差 = 1%
- 开发集误差 = 11%

从上边的数据能看出什么问题吗？应用前一节的定义，我们估计该分类器的偏差为1%，同时方差为10%（11%-1%=10%）。因此，它存在高方差(High Variance)问题。分类器的训练集误差很小，但未能把在训练集上完美表现推广到开发集中，这也被称为“过拟合”(Overfitting)。

现在，算法表现变成了下列所示：

- 训练集误差 = 15%
- 开发集误差 = 16%

我们估计该分类器的偏差达到了15%，方差是1%。这个分类器对训练集的拟合效果很差，误差居然有15%，但是它在开发集上的误差和训练集上相当（也有16%）。因此该分类器存在高偏差(High Bias​​)问题，这也被称为“欠拟合”(Underfitting)。

再来看一种情况：

- 训练集误差 = 15%
- 开发集误差 = 30%

我们估计偏差为15%，方差也达到了15%。该分类器同时存在高偏差和高方差问题：训练集的表现效果很差，因此偏差很大，同时他在开发集的表现更差，因此方差也很大。这是由于分类器模型设计的有问题，属于最糟糕的情况，欠拟合/过拟合的优化技术很难应用到这类情况中来。

再来看最后一种情况：

- 训练集误差 = 0.5%
- 开发集误差 = 1%

这个分类器表现完美，同时拥有低方差和低偏差，祝贺你取得了这样的成绩。

## 22. 比较最优误差

在我们的猫咪识别器的例子中，理想的误差，即最优分类器的误差应该接近0%。一个人类几乎总是能认出照片里边的猫来，所以，我们也希望机器能达到这个水平。

有些问题难度则更大，比如，假设你正在建立一个语言识别系统，并发现在音频片段中有14%都是背景噪声，或者是无法理解的内容，即使是人类来也无法识别出这部分信息。在这种情况下，意味着即使是“最佳的”语音识别系统也可能存在14%的误差。

假设在这个语音识别问题上，你的算法的表现如下：

- 训练集误差 = 15%
- 开发集误差 = 30%

从上边可以看出，算法在训练集上的性能表现已经接近了14%的最优误差，因此，就偏差或者训练集性能方面而言，已经没有太大的改进空间了。但是由于该算法对于开发集的拟合并不好，因此对于方差而言却存在着更大的改进空间。

这个例子类似上一节中的第三个例子（训练集误差15%，开发集误差30%）。如果最优误差是~0%，则15%的训练集误差就有很大改进空间，这表明执行减少偏差的改进措施是将会很有效果。但是，如果最优误差为14%，则相同的训练集表现告诉我们在分类器的偏差方面能改进的空间真的很小。

对于这种最优误差远远大于0%的问题，这里有关于算法误差更详细的分类。继续我们上边提到的语音识别的例子，30%的总开发集误差可细分为以下几类（类似的分类可以同时应用于对测试集误差的分析过程中）：

- 最优误差(Optimal Error Rate)：14%。假设我们定义：即使是世界上最优秀的语音系统也存在14%的误差。我们可以将这部分误差归类为学习算法偏差中“不可避免”的部分；
- 可避免偏差(Avoidable Bias)：1%。这被定义为是在训练误差和最优误差之间的差值（训练误差-最优误差）[^3]；
- 方差(Variance)：15%。这被定为是开发集误差和训练集误差之间的差值（开发集误差-训练集误差）。为了和我们之前的一些定义统一起来，偏差和可避免偏差有下列联系[^4]：
    - 偏差 = 最优误差 + 可避免偏差

[^3]:如果这个差值是负数，那么你的训练集的表现比最优分类器都还要好，意味着你的算法在训练集过拟合了，并且该算法已经“Over-Memorized”了训练集。你应该专注于减少方差，而不是继续减少偏差。

[^4]:选择这样定义是为了更好地传达关于如何改进学习算法的思想。这些定义与统计学家对这些概念的定义并不一样。从技术上来说，本文所定义的“偏差”应该被称为“我们归因于偏差的错误”(Error we attribute to bias)，“可避免偏差”应该被称为“我们归因于学习算法偏差超过了最优误差的错误”(error we attribute to the learning algorithm's bias that is over the optimal error rate)。

“可避免偏差”反映了算法在训练集上的表现与“最优分类器”比还差多少。

方差的概念和前面的一样，没有变化。因为从理论上来说，我们总是可以通过大规模训练集的训练来将方差减少到零，只要使用足够大的训练集就可以完全避免方差，因此所有的方差都是“可避免的”。

再考虑一个例子，这里我们设定最优误差是14%：

- 训练集误差 = 15%
- 开发集误差 = 16%

在前一节的内容中，我们将有上述表现的分类器称为“高偏差”分类器。而现在我们可以认为该分类器的可避免偏差只有1%，也就是说，这个算法已经很优秀，几乎没有改进的空间了，因为它在开发集上的表现仅比最优分类器差了2%而已。

从这些例子中我们可以看出，知道最优误差有助于指导我们进行后续的步骤。在统计学中，“最优误差”也被称为“贝叶斯误差”(​Bayes Error Rate)或者“贝叶斯率”

那我们怎么知道最优误差是多少呢？对于一些人类擅长的任务来说，例如识别图片或者音频片段转录等，你可以要求人类来去对训练集数据进行识别，也就是测量出人类对于这个训练集的准确率来。这将给出最优误差的估计（用人类水平误差替代最优误差）。但是如果你正在解决一个对于人类而言都很困难的任务（比如，预测要推荐什么电影或者是向用户展示何种广告），最有误差是很难用这种方法来估计的。

在“比较人类表现水平”（第33节-第35节）这一部分内容中，我将会更加详细的讨论比较学习算法表现和人类水平(Human-Level)表现的这一过程。

在本章最后几节内容中，你将学习到如何通过查看训练和开发集误差来估计可避免/不可避免的偏差和方差。下一节我们将会讨论如何通过深入的分析理解来考虑优先使用的技术，即是减少偏少的技术还是减少方差的技术。根据项目当前的问题是“高（可避免）偏差”还是“高方差”，来应用完全不同的技术手段。请继续往下读！


## 23. 解决方差和偏差

这是解决偏差和方差最简单的公式：

- 如果存在高可避免偏差问题(High Avoidable Bias)，增大你模型的规模（比如，为模型设计更多的层数和神经元）；
- 如果存在高方差问题(High Variance)，则为训练集添加更多的数据。

如果你能够增加神经网络的规模并且无限制地增加训练数据，那么可以很好地解决许多学习算法出现的问题。

实际上，盲目增加模型的规模最终会导致你遇到计算成本的问题，因为训练非常大的模型是很慢的。而你也可能会耗尽获取更多训练数据的能力。（即使在网上，关于猫咪的图片也是有限的）

不同的模型架构——例如，不同的神经网络架构——会对你的问题产生不同的大小的偏差/方差量。近期的很多深度学习的研究开发出了很多有创新性的模型架构。所以如果你正在使用神经网络，那么学术文献将是一个很好的灵感来源。同时在 Github 上也有很多开源代码。但是需要提醒的一点就是：尝试新的模型架构产生的后果将会比单纯的增大原有模型规模和增加数据量更加难以预料。

增大模型的尺寸通常会减少偏差，但同时也会有使方差增大和过拟合的风险出现。然而，这种过拟合的问题一般只在你不使用正则化(Regularization)的时候才存在，如果在你的架构中已经包含了精心设计的正则化方法，那么就可以放心的增大模型的规模而不用担心过拟合的问题。

假设你正在使用诸如L2正则化、Dropout等深度学习技术去正则化参数来让算法在开发集上有好的表现，在此基础上如果增加模型的规模，对算法来说，他的性能通常会得到改善，至少不会变差。避免使用更大规模的模型的唯一原因就是计算成本增加的问题。

## 24. 权衡偏差和方差

你可能听说过：“权衡偏差和方差”，在你可以对大多数学习算法所能作出的更改中，有一些能够减少偏差，但会增大方差，或者是在减少方差的时候却增大了偏差。这就需要在偏差和方差问题上进行“权衡(Trade Off)”。

比如，增大你模型的规模——也就是在神经网络中增大神经元或者层的数量，或者是增加输入特征——这通常会减少偏差，但可能会增大方差。另外，增加正则化措施则会增加偏差但是会减少方差。

在当代，我们可以很容易地获取到海量的数据，并且可以使用大规模的神经网络（深度学习）。因此，需要作出这种权衡的机会比以前要少，因为现在有更多的方法能够做到在减少偏差的同时保证方差的在可接受范围，反之亦然。

比如，你通常可以增加神经网络的规模并通过调整正则化方法使得在减少偏差的同时不会导致方差显著增大。通过增大训练集数据，你也可以做到减少方差的同时不影响偏差。

如果你选择的模型框架非常适合于你的任务的话，那么你也能够做到同时减少偏差和方差，不过选择出这种架构的难度可能会很大。

在接下来的几节内容中，我们将会具体讨论解决偏差和方差问题的特殊技巧。

## 25. 减少可避免偏差的技巧

如果你的学习算法遇到了高可避免偏差问题，你可以尝试以下技巧：

- 增大模型规模（增加层或者神经元数量）：此技巧能够减少偏差，因为它能够帮助你更好的去拟合训练集。如果你发现这种做法增大了方差，那么使用正则化，这通常会抑制方差的增大；
- 根据误差分析修改输入特征：假设你的误差分析促使你添加额外的特征来帮助算法消除特定类别的错误。（我们将在下一节的内容里进一步讨论）也就是说，这些新特征能有助于避免偏差和方差。从理论上来看，添加更多的特征会导致方差增大，但当你发现这种情况的时候，使用正则化吧，它能够抑制方差的增大；
- 减少或取消正则化（L2正则化、L1正则化、Dropout）：虽然正则化会减少方差，但是会增大偏差啊；
- 修改模型架构：通过修改模型的架构来让模型更契合你的问题，这通常会同时影响你的偏差和方差。

另外，需要提醒的一点就是：

- 添加更多的训练数据：此技巧往往有助于解决方差问题，但对偏差问题没什么用。

## 26. 在训练集上的误差分析

你的算法首先得在训练集上表现良好，才能谈得上在开发集/测试集上的表现。

除了前面介绍的解决高偏差的技巧之外，我有时还会对训练数据进行误差分析，遵循类似于“眼球开发集”的误差分析模式。如果你的算法有很高的偏差的话，换句话说，你的算法不能很好的适用在训练集上，这可能会很有用。

比如，假设你正在为你的应用程序构建一个语音识别系统，并收集了一批来自于志愿者的音频剪辑数据。如果你的系统在该训练集上表现不佳，你应该考虑人工听取一组（100个左右样例）算法难以处理的音频剪辑数据，从而理清训练集误差的主要类别。与开发集上的误差分析类似，你应该计算在不同类别上所犯的错误比例：

音频片段|很大的背景噪声|发言者语速过快|声源离麦克风太远|备注
-------|------|------|------|------
1|√|||汽车噪声
2|√||√|餐厅噪声
3||√|√|用户在客厅里大喊大叫
4|√|||咖啡店
占总数百分比|75%|25%|50%|

在这里例子里，你可能注意到你的算法在对具有很高背景噪声的样本进行识别的时候会特别困难。因此，你可能会专注于那些能够让你的算法很好的适应带有背景噪声的样本的技术。

你还可以进行二次检查：给定与你的算法相同的输入音频片段，让一个人去听，然后看他是否能将这些音频片段转录出来。如果背景噪声太大以至于任何人都没办法确定音频里边所说的话，那么期望有算法能够识别这些数据也是不合理的。我们将在后续章节讨论将你的算法表现和人类水平表现进行比较的好处。

## 27. 减少方差的技巧

如果你的学习算法遇到了高方差问题，你可以尝试以下技巧：

- 加入更多的训练集数据：这是用来解决方差问题最简单也是最可靠的办法。只要你能获取更多的数据和有足够的计算能力来处理这些数据，那么加吧。
- 加入正则化：（L2正则化、L1正则化、Dropout）：正则化减少方差，但是会增大偏差；
- 加入早停止(Early Stopping)（例如根据开发集误差来提前停止梯度下降）:这个技巧同样的减少了方差但是增大了偏差。这种早期停止的行为很像是正则化方法，所以有些研究者也把它归类为正则化技术；
- 进行特征筛选(Feature Selection)来减少输入特征的数量/类型：这个技巧同样可能会在有助于减少方差的同时增大偏差。稍微减少特征的数量（比如总共有1000个特征然后减少到900个）不太可能对偏差产生很大的影响。只要不把很多的主要特征给剔除掉，显著减少它（假设我们把总过1000个特征砍剩100个，减少了10倍）更可能产生明显的效果。在当今条件下的深度学习中，在我们拥有了海量的数据之后，我们的精力就已经从特征选择中转移出来了，现在我们更倾向于将我们拥有的所有特征都丢给算法，然后让算法自己根据数据去分类使用哪些特征。但是当你的训练集很小的时候，进行特征选择将会是很有用的方法；
- 降低模型的规模（层和神经元数量）：慎重使用！！！！这个技巧可以减少方差，但可能会增大偏差。但是，我不推荐使用这种方法来解决方差问题。为算法增加正则化就可以提高分类性能。减少模型的规模的最大好处是降低计算成本从而加快训练模型的速度，如果加速训练模型速度是有效果的，那么我们务必通过办法来降低其规模。但，如果你的目标是减少方差，同时并不在意计算成本的问题，请优先考虑使用正则化！

这里有两个额外的策略，与25节讨论“减少可避免偏差的技巧”重复：

- 根据误差分析修改输入特征：假设你的误差分析促使你添加额外的特征来帮助算法消除特定类别的错误。也就是说，这些新特征能有助于避免偏差和方差。从理论上来看，添加更多的特征会导致方差增大，但当你发现这种情况的时候，使用正则化吧，它能够抑制方差的增大；
- 修改模型架构：通过修改模型的架构来让模型更契合你的问题，这通常会同时影响你的偏差和方差。

---

🚧🚧🚧未完待续！🚧🚧🚧