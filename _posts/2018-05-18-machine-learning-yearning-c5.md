---
layout:     post
title:      《机器学习要领》 学习曲线（中文翻译版）
subtitle:   Machine Learning Yearning Chapter5 Learning curves(Chinese ver)
date:       2018-05-18
author:     Canary
header-img: img/ML_yearning1.jpg
catalog: true
tags:
    - maching learning yearning
    - 机器学习
    - Andrew NG
    - 翻译
---

## 前言

> 本篇博客是 Andrew NG 《Machine Learning Yearning》 的“第五章：学习曲线”翻译。本章内容将提供一个更加丰富和直观的方式，来帮助你更好地将偏差归因到可避免偏差或者是方差上。   
👉[官网传送门](http://www.mlyearning.org/)<br>
👉[GitHub项目传送门](https://github.com/AlbertHG/Machine-Learning-Yearning-Chinese-ver)，欢迎Star

## 28. 诊断偏差和方差：学习曲线

我们已经见过一些方法，并使用它们来估计有多少误差被归因到可避免偏差或者是归因到方差上。比如，我们通过估计最优误差和计算学习算法的训练集和开发集误差做到了这一点。现在，让我们来讨论一个信息量更大的方法：绘制学习曲线(Learning Curves)。

学习曲线以你的训练样本的数量为横轴，误差为纵轴。要绘制它，你可以使用不同的训练集大小来重复运行算法，得出在不同训练集大小下的开发集误差。比如，你总共有1000 个样本，则可以让算法分别在100、200、300、……、1000大小的样本尺寸下进行训练，然后绘制出在训练集大小不断增大下的开发集误差的变化趋势，如下图：

![](https://raw.githubusercontent.com/AlbertHG/alberthg.github.io/master/makedown_img/20180422mlyearning/11.png)

随着训练集大小的增加，开发集误差在下降。

“期望误差率(Desired Error Rate)”是我们希望学习算法能够最终达到的那个表现水平，例如：

- 如果我们希望达到“人类表现水平”，那么“期望误差率”指的就是人类误差率；
- 如果我们的学习算法为特定的产品提供服务（例如为猫APP提供猫咪识别器），我们可能会需要一种直觉，来发觉需要达到怎么的表现水平的算法才能给用户提供一个绝佳的体验。
- 如果你长期从事在一个关键的应用程序上，那么你可能具备这个直觉，能够预计出在下季度/下一年中能够取得多大的进展。

所以，将期望的表现水平添加到你的学习曲线中：

![](https://raw.githubusercontent.com/AlbertHG/alberthg.github.io/master/makedown_img/20180422mlyearning/12.png)

在图中，可以管观察红色的开发误差曲线，从而猜测出可以通过添加更多的数据来算法达到期望的性能水平。在上图展示的例子中，通过加倍训练集尺寸来促使学习算法朝着期望的性能改进看起来是合理的。

但是，假设开发误差曲线已经被“压平了(Plateaued)”，下图，你就能够立刻反应过来，继续添加数据已经不能够帮助你的算法往目标逼近了：

![](https://raw.githubusercontent.com/AlbertHG/alberthg.github.io/master/makedown_img/20180422mlyearning/13.png)

通过观察学习曲线可能会帮助你避免浪费几个月的时间来搜集大量的训练数据，而之后才意识到这对于优化性能没有帮助。

这个过程的一个缺点是：如果你只关注开发误差曲线，在有大量数据的情况下你很难推断和预测红色曲线的走势。所以，现在有另外一根额外的曲线可帮助你评估添加更多数据对性能的影响：训练误差曲线。

## 29. 绘制训练误差曲线

随着训练集大小的增加，你的开发集（和测试集）的误差应该会减少；但是随着训练集大小的增加，你的训练集的误差应该是会增加的。

让我们以例子来说明这种情况，假设你的训练集只有两个样本：一个猫图和一个非猫图。然后学习算法很容易的就学习到了训练集中的这两个样本，并达到了0%的训练集误差。即使其中一个或者两个训练样本的标签都被标记错误了，对于学习算法来说，仍然能够轻松的记住这两个标签。

现在假设你的训练集有100个样本，也许有一些例子时被错误标记的，或者是模棱两可——图片模糊到即使是人类也无法分辨图片中有无猫。也许学习算法仍然能够“记忆”大部分或全部训练集样本，但也很难取得100%的准确率了。通过将训练集样本量从2个增加到100个，你会发现训练集准确率会稍微下降。

最后，假设你的训练集有10000个样本，在这种情况下，算法更加难以去拟合这10000个样本了，特别是还存在模糊图片或者时标记错误图片的情况下。因此你的算法表现在训练集上会变得更差。

让我们将训练误差曲线绘制到之前的那张图上：

![](https://raw.githubusercontent.com/AlbertHG/alberthg.github.io/master/makedown_img/20180422mlyearning/14.png)

你可以看到蓝色的“训练误差曲线”是随着训练集样本量的增大而增大的。此外，你的算法的在训练集上的表现通常要好于开发集，因此，红色的开发及误差曲线一般来说会严格地高于训练误差曲线。

接下来，让我们讨论如何解读这些曲线图。

## 30. 解读学习曲线：高偏差

假设你的开发误差曲线看起来是这样的：

![](https://raw.githubusercontent.com/AlbertHG/alberthg.github.io/master/makedown_img/20180422mlyearning/15.png)

我们之前说过，如果你的开发误差曲线平稳，也就是说，无法通过添加数据来让算法达到期望的性能。

但是我们很难切确的知道红色的开发误差曲线的外推（[外推法](https://baike.baidu.com/item/%E5%A4%96%E6%8E%A8%E6%B3%95)）是怎样的。如果开发集很小，则更不确定，因为曲线可能是有噪声的。

假设我们将训练误差曲线添加到该图中并得到以下结果：

![](https://raw.githubusercontent.com/AlbertHG/alberthg.github.io/master/makedown_img/20180422mlyearning/16.png)

现在，你可以绝对确定，添加更多的数据本身是无效的。为什么？记住我们下面提到两个结论：

- 随着我们添加越来越多的训练数据，训练误差只会越来越大。因此，蓝色的训练误差曲线只能够保持当前水平或者更糟糕（曲线趋势往上），所以随着训练集数据的增多，曲线会往远离期望性能曲线（绿线）的方向发展。
- 红色的开发误差曲线通常高于蓝色训练误差曲线，意味着，当训练误差曲线都高于期望性能曲线（绿线）并有继续远离的趋势的时候，几乎不可能通过添加数据的方式来让红色的开发误差曲线下降到期望的性能水平。

在同一个图表上同时检查开发误差曲线和训练误差曲线。使我们能够更加自信的去推断开发误差曲线。

为了更好的阐述，假设我们把对最优误差的估计作为期望性能表现，那么上图就是一个教科书式的关于一个具有高可避免偏差的学习曲线的例子：在最大的训练集规模下（大概理解为对应于我们所有的训练数据），训练误差和期望误差之间存在很大的差距（进步空间），说明该案例遇到了高可避免偏差问题。此外，训练误差曲线和开发误差曲线的差距很小，意味着方差不大。

以前，我们只在该图的最右点来测量训练集和开发集的误差，这个点与我们算法使用了全部可用训练数据相对应。而绘制完整的学习曲线可以让我们更加全面的了解在不同训练集大小下的算法性能表现。

------


🚧🚧🚧未完待续！🚧🚧🚧