---
layout:     post
title:      机器学习策略(1)
subtitle:    "\"deeplearning.ai-Class3-Week1\""
date:       2018-04-12
author:     ATuk
header-img: img/deeplearning_c3_w1.jpg
catalog: true
tags:
    - 深度学习
    - 笔记
    - deeplearning.ai
    - 网易云课堂
    - Andrew NG
---

## [GitHub项目传送门](https://github.com/AlbertHG/Coursera-Deep-Learning-deeplearning.ai)

> 欢迎Star

## 为什么是ML策略(Why ML Strategy)

对于一个已经被构建好且产生初步结果的机器学习系统，为了能使结果更令人满意，往往还要进行大量的改进。鉴于之前的课程介绍了多种改进的方法，例如收集更多数据、调试超参数、调整神经网络的大小或结构、采用不同的优化算法、进行正则化等等，有针对性的调整才能事半功倍。

## 正交化(Orthogonalization)

搭建机器学习系统的挑战之一便是可以尝试和改变的东西太多了。**正交化(Orthogonalization)** 的核心在于每次调整只会影响模型某一方面的性能，而对其他功能没有影响。这种方法有助于更快更有效地进行机器学习模型的调试和优化。

![](https://raw.githubusercontent.com/AlbertHG/Coursera-Deep-Learning-deeplearning.ai/master/03-Structuring%20Machine%20Learning%20Projects/md_images/01.png)

上图左侧是一张老式电视图片，有很多旋钮可以用来调整图像的各种性质。这些老式电视可能有一个旋钮来调整图像垂直方向的高度、一个旋钮用来调节图像的宽度、一个旋钮用来调节梯形角度、一个旋钮用来调节图像的左右偏移，还有一个旋钮用来调节图像选择的角度等。电视设计师花了大量时间设计电路来确保每个旋钮都有相对明确的功能，正交化指的就是电视设计师在设计旋钮时，让每个旋钮都只调整电视的一个方面。

要设计好一个有监督学习系统，需要设计好调整系统的“旋钮”来确保下图所提及的四件事情，也就是说要确保一下四点互为正交（修改其中一条不会影响其他）:

1. 当设计的算法 **在成本函数上不能很好地拟合训练集时(训练集表现不好)** ，就应该设计一个“旋钮”确保可以调整算法使得它很好地拟合训练集。例如训练更大的网络或者切换到像 Adam 算法之类的更好的优化算法等。
2. 当设计的算法 **对训练集的拟合很好但对开发集的拟合很差时（验证(开发)集上表现不好）** ，可以使用正则化或者更好的训练集等“旋钮”来调整。
3. 当设计的算法 **对开发集的拟合很好但对测试集的拟合很差时（测试集上表现不好）** ，意味着过拟合了，可以使用更大的开发集这个“旋钮”来调整。
4. 当设计的算法 **对测试集的拟合很好但在实际情况中却无法表现得很好时（实际应用中表现不好）** ，此时可以选择改变开发集或成本函数。

## 单一数字评估指标(Single number evaluation metric)

构建机器学习系统时，通过设置一个量化的单值评价指标(single-number evaluation metric)，可以使我们根据这一指标比较不同超参数对应的模型的优劣，从而选择最优的那个模型。

评估你的分类器的一个合理方式是观察它的查准率(Precision)和查全率(Recall),对于猫分类器这个案例:

- 查准率的定义是在分类器标记为猫的例子中，有多少真的是猫。 $$\frac{预测为正类的正类数量}{预测为正类的数量} * 100% $$
- 查全率就是分类器正确识别出的真是猫的图片数量占所有的真是猫的图片数量的百分比。 $$\frac{预测为正类的正类数量}{正类数量} * 100% $$

假设我们有 A 和 B 两个分类器，其两项指标分别如下：

| 分类器 | 查准率 | 查全率 |
| :---: | :---: | :---: |
| A | 95% | 90% |
| B | 98% | 85% |

A 的查全率较高，B 的查准率较高，一时无法二选一，所以难以使用两个评估指标——查准率和查全率，来快速地选择一个分类器，这就需要找到一个新的评估指标能够结合查准率和查全率。在机器学习中，结合查准率和查全率的标准方法是所谓的 **F1 分数** ，可以认为这是查准率 P 和查全率 R 的平均值。正式来说，F1 score 的公式叫做查准率 P 和查全率 R 的调和平均数。
$$F1 = \frac{2}{\frac{1}{P}+\frac{1}{R}} = \frac{2PR}{P+R}$$

因此，我们计算出两个分类器的 F1 Score。可以看出 A 模型的效果要更好。

| 分类器 | 精确率 | 召回率 | F1 Score |
| :---: | :---: | :---: | :---: |
| A | 95% | 90% | 92.4% |
| B | 98% | 85% | 91.0% |

通过引入单值评价指标，就可以加速改进机器学习算法的迭代过程。

## 满足和优化指标(Satisficing and Optimizing metric)

有时候，要把所有的性能指标都综合在一起，构成单值评价指标是比较困难的。解决办法是，我们可以把某些性能作为优化指标(Optimizing metic)，寻求最优化值；而某些性能作为满意指标(Satisficing metic)，只要满足阈值就行了。

举个猫类识别的例子，有A，B，C三个模型，各个模型的Accuracy和Running time如下表中所示：

![](https://raw.githubusercontent.com/AlbertHG/Coursera-Deep-Learning-deeplearning.ai/master/03-Structuring%20Machine%20Learning%20Projects/md_images/02.jpg)

Accuracy和Running time这两个性能不太合适综合成单值评价指标。因此，我们可以将Accuracy作为优化指标(Optimizing metic)，将Running time作为满意指标(Satisficing metic)。也就是说，给Running time设定一个阈值，在其满足阈值的情况下，选择Accuracy最大的模型。如果设定Running time必须在100ms以内，那么很明显，模型C不满足阈值条件，首先剔除；模型B相比较模型A而言，Accuracy更高，性能更好。

概括来说，性能指标(Optimizing metic)是需要优化的，越优越好；而满意指标(Satisficing metic)只要满足设定的阈值就好了。

## 训练集、开发集、测试集的划分(Train/dev/test distributions)

训练集(Train sets)、开发(验证)集(Dev sets)、训练集(Test sets)如何设置对机器学习的模型训练非常重要，合理设置能够大大提高模型训练效率和模型质量。

机器学习的流程就是尝试很多思路，用训练集训练不同的模型，然后使用开发集来评估不同的思路，最终选择其中一个思路。之后不断地迭代去改善性能，直到得到一个令人满意的结果。最后，再用测试集去评估它。

原则上应该尽量保证Dev sets和Test sets来源于同一分布且都反映了实际样本的情况。如果Dev sets和Test sets不来自同一分布，那么我们从Dev sets上选择的“最佳”模型往往不能够在Test sets上表现得很好。这就好比我们在Dev sets上找到最接近一个靶的靶心的箭，但是我们Test sets提供的靶心却远远偏离Dev sets上的靶心，结果这支肯定无法射中Test sets上的靶心位置。

![](https://raw.githubusercontent.com/AlbertHG/Coursera-Deep-Learning-deeplearning.ai/master/03-Structuring%20Machine%20Learning%20Projects/md_images/03.jpg)

## 开发集和测试集的大小(Size of the dev and test sets)

过去数据量较小（小于 1 万）时，通常将数据集按照以下比例进行划分：

* 无验证集的情况：70% / 30%；
* 有验证集的情况：60% / 20% / 20%；

这是为了保证验证集和测试集有足够的数据。现在的机器学习时代数据集规模普遍较大，例如 100 万数据量，这时将相应比例设为：

-  有验证集的情况：98% / 1% / 1%
-  无验证集的情况：99% / 1%

就已经能保证验证集和测试集的规模足够。

对于Dev sets数量的设置，应该遵循的准则是通过Dev sets能够检测不同算法或模型的区别，以便选择出更好的模型。

对于Test sets数量的设置，应该遵循的准则是通过Test sets能够反映出模型在实际中的表现。

实际应用中，可能只有Train/Dev sets，而没有Test sets。这种情况也是允许的，只要算法模型没有对Dev sets过拟合。但是，条件允许的话，最好是有Test sets，实现无偏估计。

## 什么时候改变开发/测试集和评估指标(When to change dev/test sets and metrics)

算法模型的评价标准有时候需要根据实际情况进行动态调整，目的是让算法模型在实际应用中有更好的效果。

举个猫类识别的例子。初始的评价标准是错误率，算法A错误率为3%，算法B错误率为5%。显然，A更好一些。但是，实际使用时发现算法A会通过一些色情图片，但是B没有出现这种情况。从用户的角度来说，他们可能更倾向选择B模型，虽然B的错误率高一些。这时候，我们就需要改变之前单纯只是使用错误率作为评价标准，而考虑新的情况进行改变。例如增加色情图片的权重，增加其代价。

原来的Cost function：

$$J = \frac{1}{m}\sum_{m}^{i=1}L(\hat{y}^{(i)},y^{(i)})$$

更改评价标准后的Cost function：

$$J = \frac{1}{w^{(i)}}\sum_{m}^{i=1}w^{(i)}L(\hat{y}^{(i)},y^{(i)}),where\begin{cases} 1&\text{ if } x^{(i)} =non-porn \\ 10&\text{ if } x^{(i)} =porn \end{cases}$$

这样通过设置权重，当算法将色情图片分类为猫时，误差项会快速变大。

应该将机器学习任务看成两个独立的步骤:

1. 用打靶来打个比方，第一步就是设定目标，这是独立的一步。(定义一个指标来评估分类器)
2. 第二步则是瞄准和射击目标，即如何精确瞄准和如何命中目标。

## 为什么是人的表现(Why human-level performance?)

机器学习模型的表现通常会跟人类水平表现作比较，如下图所示：

![](https://raw.githubusercontent.com/AlbertHG/Coursera-Deep-Learning-deeplearning.ai/master/03-Structuring%20Machine%20Learning%20Projects/md_images/4.jpg)

上图展示了随着时间的推进，机器学习系统和人的表现水平的变化。一般的，当机器学习超过人的表现水平后，它的进步速度逐渐变得缓慢，最终性能无法超过某个理论上限，这个上限被称为 **贝叶斯最优误差(Bayes Optimal Error)** 。

贝叶斯最优误差一般认为是理论上可能达到的最优误差，换句话说，其就是理论最优函数，任何从 x 到精确度 y 映射的函数都不可能超过这个值。例如，对于语音识别，某些音频片段嘈杂到基本不可能知道说的是什么，所以完美的识别率不可能达到 100%。

因为人类对于一些自然感知问题的表现水平十分接近贝叶斯最优误差，所以当机器学习系统的表现超过人类后，就没有太多继续改善的空间了。

也因此，只要建立的机器学习模型的表现还没达到人类的表现水平时，就可以通过各种手段来提升它。例如采用人工标记过的数据进行训练，通过人工误差分析了解为什么人能够正确识别，或者是进行偏差、方差分析。

当模型的表现超过人类后，这些手段起的作用就微乎其微了。

## 可避免偏差(Avoidable bias)

模型在训练集上的误差与人类表现水平的差值被称作可避免偏差(Avoidable Bias)

假设针对两个问题分别具有相同的训练误差和交叉验证误差，如下所示：

![](https://raw.githubusercontent.com/AlbertHG/Coursera-Deep-Learning-deeplearning.ai/master/03-Structuring%20Machine%20Learning%20Projects/md_images/5.jpg)

对于左边的问题，人类的误差为 1% ，对于右边的问题，人类的误差为 7.5% 。

对于某些任务如计算机视觉上，人类能够做到的水平和贝叶斯误差相差不远。（这里贝叶斯误差指最好的分类器的分类误差，也就是说没有分类器可以做到 100% 正确）。这里将人类水平误差近似为贝叶斯误差。

- 左边的例子： 8% 与 1% 差距较大

主要着手减少偏差，即减少训练集误差和人类水平误差之间的差距，来提高模型性能,比如训练更大的神经网络或者运行梯度下降更久一点。

- 右边的例子： 8% 与 7.5% 接近

主要着手减少方差，即减少开发集误差和测试集误差之间的差距，来提高模型性能,比如正则化或者收集更多的训练数据使得开发误差更接近训练误差。

也就是说：通常，我们把训练集误差与人类水平误差之间的差值称为偏差(bias)，也称作可避免偏差(avoidable bias)；把开发集误差与训练集误差之间的差值称为方差(variance)。

## 理解人类的表现(Understanding human-level performance)

![](https://raw.githubusercontent.com/AlbertHG/Coursera-Deep-Learning-deeplearning.ai/master/03-Structuring%20Machine%20Learning%20Projects/md_images/06.png)

比如在上图的例子中:

- 第一个例子的训练误差是 5%，开发误差是 6%，而人类水平误差根据需要可以定义为 1%、0.7%或 0.5%。不论如何定义人类水平误差，在第一个例子中，它的可避免偏差大概是 4%，衡量方差的指标是 1%。此时，明显可避免偏差问题更大，在这种情况下，应该专注于减少偏差的技术，例如训练更大的网络。

- 在第二个例子中，训练误差是1%，开发误差是 5%，所以可避免偏差大概是 0%到 0.5%，衡量方差的指标是 4%，此时方差问题更大，应该主要使用减少方差的工具，比如正则化或者去获取更大的训练集。

- 在第三个例子中，训练误差是 0.7%，开发误差是 0.8%，此时选择哪个人类水平误差来估计贝叶斯最优误差就会对之后所采取的策略产生较大的影响。例如将人类水平误差定义为 0.5%，那么可避免偏差就是衡量方差的指标的两倍，这表明也许偏差和方差都存在问题，但可避免偏差问题更严重。但如果使用 0.7%来代替贝叶斯最优误差，那么会得到可避免偏差是 0%，这就可能导致忽略可避免偏差，而实际上应该继续尝试能不能在训练集上做得更好。

由上述例子就可以理解为什么在机器学习问题上取得进展会变得越来越难。因为当接近人类水平之后，想取得进展就变得很困难。例如在上面例子中，一旦接近了人类水平，那么就很难去判断是否应该继续去拟合训练集。 当然，这种问题只会出现在算法已经做得很好的情况下。例如在上图左侧的两个例子中，它们远离人类水平，此时选择将优化目标放在偏差还是方差上就可能更容易一些。这也说明了为什么接近人类水平之后，很难去分辨出问题是出在偏差上还是方差上。

![](https://raw.githubusercontent.com/AlbertHG/Coursera-Deep-Learning-deeplearning.ai/master/03-Structuring%20Machine%20Learning%20Projects/md_images/07.png)

## 超越人类的表现(Surpassing human-level performance)

现在如果一个人类专家团和单个人类专家的表现和以前一样，但是算法可以得到 0.3%的训练误差还有 0.4%的开发误差。这是否意味着过拟合了 0.2%，又或者贝叶斯误差其实是更小的值？

在这种情况下，其可避免偏差就很难知晓是多少，同时也就没有足够的信息来判断优化算法应该专注减少偏差还是减少方差。这就会使得算法取得进展的效率降低。另外比如说算法的误差已经比人类专家的误差更低，那么依靠人类的直觉来判断算法还能往什么方向优化就很困难。所以，在这个例子中，一旦算法的误差小于 0.5%，那么就没有明确的选项和前进的方向了。但这并不意味着就不能取得进展，只是一些帮助指明方向的工具就没那么好用。

## 改善你的模型表现(Improving your model performance)

想让一个监督学习算法达到使用程度，应该做到以下两点：

1. 算法对训练集的拟合很好，可以看作可避免偏差很低；
2. 如果算法在训练集上表现得很好，那么就将其推广到开发集和测试集，使得算法在这两个数据集上也表现得很好，也就是说方差不是太大。

![](https://raw.githubusercontent.com/AlbertHG/Coursera-Deep-Learning-deeplearning.ai/master/03-Structuring%20Machine%20Learning%20Projects/md_images/08.png)

总结一下本章到目前为止的内容：如果想要提升机器学习系统的性能：

1. 那么首先计算训练误差与贝叶斯最优误差估计值之间的差值，得到可避免偏差的大小。
2. 然后计算开发集误差与训练误差之间的差值，这可以衡量方差问题的严重性。
    - 通过它们直接的对比，如果发现更需要尽可能地减少可避免偏差，那么可以使用例如构建规模更大的模型、训练的时间更长、使用更好的优化算法（比如 momentum、RMSprop 或者 Adam）、寻找更好的新神经网络架构或更好的超参数等策略。
    - 另外，如果当发现方差问题严重时，可以使用例如收集更多的数据、正则化（包括 L2 正则化、Dropout 或数据增强）、试用不同的神经网络架构、超参数搜索等技巧。
